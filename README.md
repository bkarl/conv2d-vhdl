
# conv2d
 
This is an implementation of a 2D convolution in VHDL.  

## Features

* stride 1

* 3x3 filters

* zero padding

* one pixel per clock input/output

* re loadable parameters

* input image size can be changed on runtime (max image size os fixed)

* quadratic images only

## Focus

* low resource usage

* short critical path

* no vendor libraries

* bit depths and max image size can be changed by generics  
  

## Implementation

To achieve one pixel per clock input and output rate 9 multiply-accumulate operations (because of the 3x3 filter size) have to be processed in parallel. Two line buffers (module pixbuf.vhd) are used to store intermediate results of the computation.


The convolution operation can be written as

![P_{c(0,0)}=P_{(0,0)}c_0 + P_{(0,1)}c_1 + P_{(0,2)}c_2+P_{(1,0)}c_3+P_{(1,1)}c_4+P_{(1,2)}c_5+P_{(2,0)}c_6+P_{(2,1)}c_7+P_{(2,2)}c_8](https://render.githubusercontent.com/render/math?math=P_%7Bc(0%2C0)%7D%3DP_%7B(0%2C0)%7Dc_0%20%2B%20P_%7B(0%2C1)%7Dc_1%20%2B%20P_%7B(0%2C2)%7Dc_2%2BP_%7B(1%2C0)%7Dc_3%2BP_%7B(1%2C1)%7Dc_4%2BP_%7B(1%2C2)%7Dc_5%2BP_%7B(2%2C0)%7Dc_6%2BP_%7B(2%2C1)%7Dc_7%2BP_%7B(2%2C2)%7Dc_8)

Where ![c_0](https://render.githubusercontent.com/render/math?math=c_0) to ![c_8](https://render.githubusercontent.com/render/math?math=c_0) are the filter coefficients from the top left to the bottom right and ![P_{(y,x)}](https://render.githubusercontent.com/render/math?math=P_%7B(y%2Cx)%7D) is the input pixel value at the location ![(y,x)](https://render.githubusercontent.com/render/math?math=(y%2Cx)).

This computation is now split into ![y_{(y,x)}](https://render.githubusercontent.com/render/math?math=y_%7B(y%2Cx)%7D) terms to model the pixel wise input. If we get an input pixel we can immediately compute the first term.

![y_{(0,0)}=P_{(0,0)}c_0](https://render.githubusercontent.com/render/math?math=y_%7B(0%2C0)%7D%3DP_%7B(0%2C0)%7Dc_0)

In the next clock cycle when we get the second pixel we can calculate

![y_{(0,1)}=P_{(0,1)}c_1+y_{(0,0})](https://render.githubusercontent.com/render/math?math=y_%7B(0%2C1)%7D%3DP_%7B(0%2C1)%7Dc_1%2By_%7B(0%2C0%7D))

and after 3 Pixels seen

![y_{(0,2)}=P_{(0,2)}c_2+y_{(0,1})](https://render.githubusercontent.com/render/math?math=y_%7B(0%2C2)%7D%3DP_%7B(0%2C2)%7Dc_2%2By_%7B(0%2C1%7D))

![y_{(0,2)}](https://render.githubusercontent.com/render/math?math=y_%7B(0%2C2)%7D) will now be stored in the line buffer until ![P_{(1,0)}](https://render.githubusercontent.com/render/math?math=P_%7B(1%2C0)%7D) is fed to the input and we can calculate

![y_{(1,0)}=P_{(1,0)}c_3+y_{(0,2})](https://render.githubusercontent.com/render/math?math=y_%7B(1%2C0)%7D%3DP_%7B(1%2C0)%7Dc_3%2By_%7B(0%2C2%7D))

and so on. Our final value will then be ![y_{(2,2)}](https://render.githubusercontent.com/render/math?math=y_%7B(2%2C2)%7D) that contains y-terms and will be output.

To achieve maximum throughput of course all of these calculations mentioned above have to be executed in parallel.

## Integration

There is an example how to use the module in the testbench (conv2d_tb.vhd). In order to start convolution the first line of zeros (padding) has to be fed into the module. Zero values also have to be applied on the borders the the left and right of an input image.

The last row of zeros for the bottom zero padding will be generated by the module.
For each pixel that that is fed to the module the current pixels address (x/y) and a valid signal has to be assigned.
## Parameters 
### NUM_BITS_PIXEL
This is the number of bits per input pixel. To keep precision the module does all arithmetic operations using NUM_BITS_PIXEL*2 arithmetic.
### NUM_BITS_ADDR
Number of bits to encode the pixel addresses x and y. Basically lb(MAX_IMAGE_WIDTH).
### NUM_BITS_COEFF
Word width of the coefficients.
### MAX_IMG_WIDTH
Max image width for the block to expect. Images must be quadratic.
### MAX_IMG_HEIGHT
Max image width for the block to expect. Images must be quadratic.
## Test
A test file generated by a python module (data_py.bin) can be checked against a file generated by the testbench of the conv2d module (data_sim.bin).

## Resource utilization
Because of the 9 parallel MACs the module needs 9 DSPs. The maximum image size and the internal bit width defines the number of BRAM instances the synthesis tool will infer.
The memory in bits needed can be calculated using the formula
![D_{Linebuffer}=2\cdot MAX\_IMG\_WIDTH \cdot 2 \cdot NUM\_BITS\_PIXEL\ b](https://render.githubusercontent.com/render/math?math=D_%7BLinebuffer%7D%3D2%5Ccdot%20MAX%5C_IMG%5C_WIDTH%20%5Ccdot%202%20%5Ccdot%20NUM%5C_BITS%5C_PIXEL%5C%20b)
Using the parameters ![MAX\_IMG\_WIDTH=64](https://render.githubusercontent.com/render/math?math=MAX%5C_IMG%5C_WIDTH%3D64) and ![MAX\_IMG\_WIDTH=16](https://render.githubusercontent.com/render/math?math=MAX%5C_IMG%5C_WIDTH%3D16) a standard synthesis in Vivado 2019.2 for 7 Series FPGAs boils down to
* 256 LUTs 
* 1 BRAM 
* 9 DSP


## Todo

* write a wrapper for an on chip bus (AXI Stream...) for easier integration

* add different strides/paddings/filtersizes that can be changed on the fly
